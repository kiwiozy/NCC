"""
Django management command to import images from FileMaker to S3 and Nexus.

This command:
1. Authenticates with FileMaker Data API
2. Finds images where NexusExportDate is empty (not yet exported)
3. Downloads images using WATERFALL strategy (tries largest size first)
4. Uploads to S3 with organized folder structure
5. Creates ImageBatch and Image records in Nexus
6. Updates NexusExportDate in FileMaker to prevent re-import

Waterfall Strategy:
- Try image_Full first (best quality)
- If empty, try image_Ex_large
- If empty, try image_large
- If empty, try image_medium
- If empty, try image_small
- If all empty, skip image

Run: python manage.py import_filemaker_images
"""

import os
import sys
import json
import requests
import base64
from datetime import datetime, date
from io import BytesIO
from collections import defaultdict
from django.core.management.base import BaseCommand
from django.contrib.contenttypes.models import ContentType
from django.db import transaction

from patients.models import Patient
from images.models import ImageBatch, Image
from documents.services import S3Service
from PIL import Image as PILImage
from django.core.files.uploadedfile import InMemoryUploadedFile

# Suppress SSL warnings
requests.packages.urllib3.disable_warnings()

# Load FileMaker credentials from scripts/filemaker/.env
# Hardcode the project root to avoid path calculation issues
PROJECT_ROOT = '/Users/craig/Documents/nexus-core-clinic'
SCRIPT_DIR = os.path.join(PROJECT_ROOT, 'scripts', 'filemaker')
ENV_PATH = os.path.join(SCRIPT_DIR, '.env')

FM_USERNAME = None
FM_PASSWORD = None
FM_DATABASE = 'WEP-DatabaseV2'
FM_BASE_URL_DATA_API = None

if os.path.exists(ENV_PATH):
    with open(ENV_PATH, 'r') as f:
        for line in f:
            line = line.strip()
            if line.startswith('FM_USERNAME='):
                FM_USERNAME = line.split('=', 1)[1].strip().strip('"').strip("'")
            elif line.startswith('FM_PASSWORD='):
                FM_PASSWORD = line.split('=', 1)[1].strip().strip('"').strip("'")
            elif line.startswith('FM_DATABASE='):
                FM_DATABASE = line.split('=', 1)[1].strip().strip('"').strip("'")
            elif line.startswith('FM_BASE_URL_DATA_API='):
                FM_BASE_URL_DATA_API = line.split('=', 1)[1].strip().strip('"').strip("'")

if not FM_BASE_URL_DATA_API:
    FM_BASE_URL_DATA_API = f"https://walkeasy.fmcloud.fm/fmi/data/v1/databases/{FM_DATABASE}"

# Global stats for logging
stats = {
    'total_found': 0,
    'processed': 0,
    'batches_created': 0,
    'images_imported': 0,
    'skipped': 0,
    'failed': 0,
    'errors': [],
    'container_field_usage': defaultdict(int)  # Track which container fields were used
}


class FileMakerAPI:
    def __init__(self):
        self.token = None
        self.layout = "API_Images"
        self.last_auth_time = None
        self.token_lifetime = 900  # 15 minutes (FileMaker default is 15 min)
        
        # Container fields in priority order (waterfall)
        self.container_fields = [
            'image_Full',      # Best quality
            'image_Ex_large',  # Fallback 1
            'image_large',     # Fallback 2
            'image_medium',    # Fallback 3
            'image_small'      # Last resort
        ]

    def ensure_authenticated(self):
        """Ensure we have a valid token, re-authenticating if needed"""
        import time
        
        # If no token, authenticate
        if not self.token:
            return self.authenticate()
        
        # If token is about to expire (within 2 minutes), re-authenticate
        if self.last_auth_time:
            elapsed = time.time() - self.last_auth_time
            if elapsed > (self.token_lifetime - 120):  # Refresh 2 min before expiry
                print(f"   üîÑ Token expiring soon ({elapsed:.0f}s), re-authenticating...")
                self.logout()
                return self.authenticate()
        
        return True

    def authenticate(self):
        """Authenticate with FileMaker Data API"""
        import time
        
        if not FM_USERNAME or not FM_PASSWORD:
            print(f"‚ùå Missing credentials: FM_USERNAME={bool(FM_USERNAME)}, FM_PASSWORD={bool(FM_PASSWORD)}")
            return False
            
        auth_url = f"{FM_BASE_URL_DATA_API}/sessions"
        auth_string = base64.b64encode(f"{FM_USERNAME}:{FM_PASSWORD}".encode()).decode()
        headers = {
            'Authorization': f'Basic {auth_string}',
            'Content-Type': 'application/json'
        }
        try:
            response = requests.post(auth_url, headers=headers, timeout=30, verify=False)
            if response.status_code == 200:
                self.token = response.json()["response"]["token"]
                self.last_auth_time = time.time()
                print(f"‚úÖ Authentication successful")
                return True
            else:
                print(f"‚ùå Authentication failed: {response.status_code} - {response.text}")
                return False
        except Exception as e:
            print(f"‚ùå Authentication error: {e}")
            return False

    def logout(self):
        """Logout from FileMaker Data API"""
        if self.token:
            logout_url = f"{FM_BASE_URL_DATA_API}/sessions/{self.token}"
            headers = {'Authorization': f'Bearer {self.token}'}
            try:
                requests.delete(logout_url, headers=headers, timeout=10, verify=False)
            except Exception:
                pass

    def find_unexported_images(self, limit=100, offset=1):
        """Find images in FileMaker that have not been exported yet"""
        # Ensure token is valid before making request
        if not self.ensure_authenticated():
            return [], {}
        
        find_url = f"{FM_BASE_URL_DATA_API}/layouts/{self.layout}/_find"
        headers = {
            'Authorization': f'Bearer {self.token}',
            'Content-Type': 'application/json'
        }
        
        # Search for images where NexusExportDate is empty
        search_data = {
            "query": [
                {"NexusExportDate": "="}  # "=" finds empty fields in FileMaker
            ],
            "offset": str(offset),
            "limit": str(limit)
        }
        
        try:
            response = requests.post(find_url, headers=headers, json=search_data, timeout=30, verify=False)
            
            if response.status_code == 200:
                data = response.json()
                records = data.get('response', {}).get('data', [])
                data_info = data.get('response', {}).get('dataInfo', {})
                return records, data_info
            else:
                return [], {}
        except Exception:
            return [], {}

    def get_best_image_container(self, field_data):
        """
        Waterfall strategy: Try each container field in order until we find one with data.
        
        Returns: (field_name, container_url) or (None, None) if all empty
        """
        for field_name in self.container_fields:
            container_url = field_data.get(field_name)
            if container_url and isinstance(container_url, str) and container_url.startswith('http'):
                # Found a container field with data!
                stats['container_field_usage'][field_name] += 1
                return field_name, container_url
        
        # All container fields are empty
        return None, None

    def download_container_file(self, url):
        """Download file from container URL using the current token"""
        # Ensure token is valid before making request
        if not self.ensure_authenticated():
            return None, None
        
        headers = {'Authorization': f'Bearer {self.token}'}
        try:
            response = requests.get(url, headers=headers, stream=True, timeout=60, verify=False)
            response.raise_for_status()
            return response.content, response.headers.get('Content-Type')
        except requests.exceptions.RequestException:
            return None, None

    def update_export_date(self, fm_record_id):
        """Update NexusExportDate in FileMaker for a given record"""
        # Ensure token is valid before making request
        if not self.ensure_authenticated():
            return False
        
        update_url = f"{FM_BASE_URL_DATA_API}/layouts/{self.layout}/records/{fm_record_id}"
        headers = {
            'Authorization': f'Bearer {self.token}',
            'Content-Type': 'application/json'
        }
        update_data = {
            "fieldData": {
                "NexusExportDate": datetime.now().isoformat()
            }
        }
        try:
            response = requests.patch(update_url, headers=headers, json=update_data, timeout=30, verify=False)
            return response.status_code == 200
        except Exception:
            return False


def get_patient_nexus_id(filemaker_contact_id):
    """
    Find the Nexus patient UUID given the FileMaker contact ID.
    
    The FileMaker ID is stored in the patient.notes field as a JSON string:
    notes = '{"filemaker_id": "UUID-HERE", ...}'
    
    Since notes is a TextField (not JSONField), we use string contains.
    """
    if not filemaker_contact_id:
        return None
    
    try:
        # Use __contains for string search within the JSON text
        patient = Patient.objects.filter(
            notes__contains=f'"filemaker_id": "{filemaker_contact_id}"'
        ).first()
        if patient:
            return str(patient.id)
        return None
    except Exception:
        return None


def process_images_for_patient(fm_api, patient_images, nexus_patient_id, s3_service, command):
    """
    Process all images for a single patient, grouped by date into batches.
    
    Args:
        fm_api: FileMaker API instance
        patient_images: List of image records for this patient
        nexus_patient_id: Nexus patient UUID (or None for unlinked)
        s3_service: S3Service instance
        command: Django management command instance for stdout
    
    Returns:
        Number of images successfully imported
    """
    # Group images by date
    images_by_date = defaultdict(list)
    
    for img_record in patient_images:
        field_data = img_record.get('fieldData', {})
        image_date_str = field_data.get('date')
        
        if image_date_str:
            images_by_date[image_date_str].append(img_record)
        else:
            # No date - group as "Unknown Date"
            images_by_date['unknown'].append(img_record)
    
    total_imported = 0
    
    # Create a batch for each date
    for image_date_str, images_list in images_by_date.items():
        # Parse date
        if image_date_str == 'unknown':
            captured_date = None
            batch_name = "Unknown Date (FileMaker Import)"
        else:
            try:
                captured_date = datetime.strptime(image_date_str, '%Y-%m-%d').date()
                batch_name = f"{captured_date.strftime('%d %b %Y')} (FileMaker Import)"
            except ValueError:
                captured_date = None
                batch_name = f"{image_date_str} (FileMaker Import)"
        
        command.stdout.write(f"\n   üìÅ Creating batch: {batch_name} ({len(images_list)} images)")
        
        # Create ImageBatch
        try:
            with transaction.atomic():
                if nexus_patient_id:
                    patient = Patient.objects.get(id=nexus_patient_id)
                    patient_content_type = ContentType.objects.get_for_model(Patient)
                    
                    batch = ImageBatch.objects.create(
                        content_type=patient_content_type,
                        object_id=nexus_patient_id,
                        name=batch_name,
                        description=f"Imported from FileMaker on {datetime.now().strftime('%Y-%m-%d')}"
                    )
                else:
                    # Unlinked batch - we'll handle this differently
                    # For now, skip unlinked images (can revisit later)
                    command.stdout.write(f"      ‚è≠Ô∏è  Skipping batch (no patient link)")
                    stats['skipped'] += len(images_list)
                    continue
                
                stats['batches_created'] += 1
                
                # Process each image in this batch
                for img_record in images_list:
                    field_data = img_record.get('fieldData', {})
                    fm_image_id = field_data.get('id')
                    fm_record_id = img_record.get('recordId')
                    
                    # Get best available container field (waterfall)
                    container_field, container_url = fm_api.get_best_image_container(field_data)
                    
                    if not container_url:
                        command.stdout.write(f"      ‚è≠Ô∏è  Skipping image {fm_image_id}: All container fields empty")
                        stats['skipped'] += 1
                        continue
                    
                    command.stdout.write(f"      üì• Downloading: {fm_image_id} (from {container_field})")
                    
                    # Download image
                    file_content, mime_type = fm_api.download_container_file(container_url)
                    if not file_content:
                        command.stdout.write(f"      ‚ùå Download failed: {fm_image_id}")
                        stats['failed'] += 1
                        stats['errors'].append(f"{fm_image_id}: Download failed from {container_field}")
                        continue
                    
                    # Get metadata
                    image_type = field_data.get('Type', 'Unknown')
                    original_filename = field_data.get('Name of file', f'{fm_image_id}.jpg')
                    
                    # Determine file extension
                    import mimetypes
                    ext = mimetypes.guess_extension(mime_type) if mime_type else '.jpg'
                    if not original_filename.endswith(ext):
                        original_filename = f"{os.path.splitext(original_filename)[0]}{ext}"
                    
                    # Upload to S3 (using same structure as documents import)
                    # S3 path: patients/filemaker-import/images/{patient_id}/{date}/{filemaker_id}.jpg
                    try:
                        # Open image with Pillow to get dimensions and generate thumbnail
                        file_obj = BytesIO(file_content)
                        img = PILImage.open(file_obj)
                        width, height = img.size
                        
                        # Generate thumbnail (300x300 max, same as user uploads)
                        thumb_img = img.copy()
                        thumb_img.thumbnail((300, 300), PILImage.Resampling.LANCZOS)
                        
                        # Save thumbnail to BytesIO
                        thumb_buffer = BytesIO()
                        thumb_img.save(thumb_buffer, format=img.format or 'JPEG', quality=85)
                        thumb_buffer.seek(0)
                        thumbnail_size = thumb_buffer.getbuffer().nbytes
                        
                        # Wrap thumbnail in InMemoryUploadedFile for S3 upload
                        thumb_file = InMemoryUploadedFile(
                            thumb_buffer,
                            None,
                            f"{original_filename}_thumb",
                            mime_type or 'image/jpeg',
                            thumbnail_size,
                            None
                        )
                        
                        # Generate S3 keys (matching documents structure + thumbnail)
                        date_folder = image_date_str if image_date_str != 'unknown' else 'unknown-date'
                        base_key = f"patients/filemaker-import/images/{nexus_patient_id}/{date_folder}/{fm_image_id}"
                        s3_key = f"{base_key}{ext}"
                        s3_thumbnail_key = f"{base_key}_thumb{ext}"
                        
                        # Reset file pointer for full image upload
                        file_obj.seek(0)
                        file_obj.name = original_filename
                        file_obj.size = len(file_content)
                        
                        # Upload full image to S3
                        s3_upload_info = s3_service.upload_file(
                            file_obj=file_obj,
                            filename=original_filename,
                            folder=s3_key
                        )
                        
                        # Upload thumbnail to S3
                        s3_thumb_info = s3_service.upload_file(
                            file_obj=thumb_file,
                            filename=f"{original_filename}_thumb",
                            folder=s3_thumbnail_key
                        )
                        
                        # Create Image record with thumbnail data
                        Image.objects.create(
                            batch=batch,
                            s3_key=s3_upload_info['s3_key'],
                            s3_thumbnail_key=s3_thumb_info['s3_key'],
                            original_name=original_filename,
                            file_size=len(file_content),
                            thumbnail_size=thumbnail_size,
                            mime_type=mime_type or 'image/jpeg',
                            width=width,
                            height=height,
                            category=image_type,  # Preserve exact FileMaker category
                            filemaker_id=fm_image_id,
                            uploaded_by=None  # FileMaker imports have no user
                        )
                        
                        # Update FileMaker
                        fm_api.update_export_date(fm_record_id)
                        
                        stats['images_imported'] += 1
                        total_imported += 1
                        
                        command.stdout.write(f"      ‚úÖ Imported: {original_filename} ({width}x{height}, thumb: {thumb_img.size[0]}x{thumb_img.size[1]}) from {container_field}")
                        
                    except Exception as e:
                        command.stdout.write(f"      ‚ùå Failed to save: {fm_image_id} - {e}")
                        stats['failed'] += 1
                        stats['errors'].append(f"{fm_image_id}: Save failed - {e}")
                        continue
        
        except Exception as e:
            command.stdout.write(f"   ‚ùå Batch creation failed: {e}")
            stats['failed'] += len(images_list)
            continue
    
    return total_imported


class Command(BaseCommand):
    help = 'Imports FileMaker images to S3 and Nexus using waterfall container field strategy'

    def add_arguments(self, parser):
        parser.add_argument(
            '--limit',
            type=int,
            default=None,
            help='Maximum number of images to import in this run (default: no limit)'
        )
        parser.add_argument(
            '--batch-size',
            type=int,
            default=50,
            help='Number of records to fetch per API request (default: 50)'
        )

    def handle(self, *args, **options):
        limit = options.get('limit')
        batch_size = options.get('batch_size', 50)
        self.stdout.write("=" * 80)
        self.stdout.write("üì∑ FileMaker Images to S3 Bulk Import")
        self.stdout.write("=" * 80)
        self.stdout.write(f"Database: {FM_DATABASE}")
        self.stdout.write(f"Layout: API_Images")
        self.stdout.write(f"Query: WHERE NexusExportDate IS EMPTY")
        if limit:
            self.stdout.write(f"Limit: {limit} images (batch import mode)")
        else:
            self.stdout.write(f"Limit: No limit (import all)")
        self.stdout.write(f"Batch Size: {batch_size} records per API request")
        self.stdout.write("")
        self.stdout.write("Waterfall Strategy:")
        self.stdout.write("  1. Try image_Full (best quality)")
        self.stdout.write("  2. If empty ‚Üí try image_Ex_large")
        self.stdout.write("  3. If empty ‚Üí try image_large")
        self.stdout.write("  4. If empty ‚Üí try image_medium")
        self.stdout.write("  5. If empty ‚Üí try image_small")
        self.stdout.write("")
        
        # Initialize services
        fm_api = FileMakerAPI()
        s3_service = S3Service()
        
        # Authenticate
        if not fm_api.authenticate():
            self.stdout.write("‚ùå Authentication failed - exiting")
            sys.exit(1)
        
        self.stdout.write("")
        self.stdout.write("=" * 80)
        self.stdout.write("üîç Finding and processing unexported images...")
        self.stdout.write("=" * 80)
        
        # Process in batches
        offset = 1
        total_processed = 0
        images_imported_count = 0  # Track actual imported images for limit
        
        # Group images by patient as we fetch them
        patient_images_map = defaultdict(list)
        
        while True:
            # Check if we've reached the limit
            if limit and images_imported_count >= limit:
                self.stdout.write(f"\n   ‚èπÔ∏è  Reached import limit ({limit} images) - stopping")
                break
            
            self.stdout.write(f"\nüìã Fetching batch (offset: {offset}, limit: {batch_size})...")
            records, data_info = fm_api.find_unexported_images(limit=batch_size, offset=offset)
            
            if not records:
                self.stdout.write("   ‚úÖ No more records to process")
                break
            
            found_count = data_info.get('foundCount', 0)
            returned_count = len(records)
            
            if offset == 1:
                stats['total_found'] = found_count
                if limit:
                    self.stdout.write(f"   üìä Total unexported images: {found_count} (will import up to {limit})")
                else:
                    self.stdout.write(f"   üìä Total unexported images: {found_count}")
            
            self.stdout.write(f"   üìÑ Grouping {returned_count} images by patient...")
            
            # Group records by patient
            for record in records:
                field_data = record.get('fieldData', {})
                fm_contact_id = field_data.get('id_Contact')
                
                if fm_contact_id:
                    patient_images_map[fm_contact_id].append(record)
                else:
                    patient_images_map['unlinked'].append(record)
            
            total_processed += returned_count
            stats['processed'] += returned_count
            
            # Check if we've processed all
            if returned_count == 0:
                self.stdout.write("\n   ‚úÖ No more images available from FileMaker")
                break
            
            if total_processed >= stats['total_found']:
                self.stdout.write("\n   ‚úÖ All images processed!")
                break
            
            offset += batch_size
        
        # Now process each patient's images
        self.stdout.write("\n" + "=" * 80)
        self.stdout.write(f"üìÅ Processing {len(patient_images_map)} patients' images...")
        self.stdout.write("=" * 80)
        
        for i, (fm_contact_id, images) in enumerate(patient_images_map.items(), 1):
            # Check limit before processing each patient
            if limit and images_imported_count >= limit:
                self.stdout.write(f"\n   ‚èπÔ∏è  Reached import limit ({limit} images) - stopping patient processing")
                break
            
            self.stdout.write(f"\n[{i}/{len(patient_images_map)}] Patient: {fm_contact_id}")
            self.stdout.write(f"   Images: {len(images)}")
            
            # Get Nexus patient ID
            nexus_patient_id = None if fm_contact_id == 'unlinked' else get_patient_nexus_id(fm_contact_id)
            
            if not nexus_patient_id and fm_contact_id != 'unlinked':
                self.stdout.write(f"   ‚ö†Ô∏è  No matching Nexus patient found for {fm_contact_id}")
                self.stdout.write(f"   ‚è≠Ô∏è  Skipping {len(images)} images")
                stats['skipped'] += len(images)
                continue
            
            # If we have a limit, check if we should process all images or just some
            images_to_process = images
            if limit and images_imported_count + len(images) > limit:
                # Only process enough to reach the limit
                images_to_process = images[:limit - images_imported_count]
                self.stdout.write(f"   ‚ö†Ô∏è  Limit reached - processing only {len(images_to_process)} of {len(images)} images")
            
            # Process images for this patient
            imported = process_images_for_patient(fm_api, images_to_process, nexus_patient_id, s3_service, self)
            images_imported_count += imported
            self.stdout.write(f"   ‚úÖ Imported {imported} images (total so far: {images_imported_count})")
            
            # If we skipped some images due to limit, update stats
            if len(images_to_process) < len(images):
                skipped_count = len(images) - len(images_to_process)
                self.stdout.write(f"   ‚è≠Ô∏è  Skipped remaining {skipped_count} images (limit reached)")
                stats['skipped'] += skipped_count
        
        # Logout
        fm_api.logout()
        
        # Final summary
        self.stdout.write("")
        self.stdout.write("=" * 80)
        self.stdout.write("üìä IMPORT COMPLETE")
        self.stdout.write("=" * 80)
        self.stdout.write(f"Total found: {stats['total_found']}")
        self.stdout.write(f"Processed: {stats['processed']}")
        self.stdout.write(self.style.SUCCESS(f"‚úÖ Batches created: {stats['batches_created']}"))
        self.stdout.write(self.style.SUCCESS(f"‚úÖ Images imported: {stats['images_imported']}"))
        self.stdout.write(f"‚è≠Ô∏è  Skipped: {stats['skipped']}")
        self.stdout.write(self.style.ERROR(f"‚ùå Failed: {stats['failed']}"))
        
        # Container field usage stats
        self.stdout.write("")
        self.stdout.write("üìä Container Field Usage (Waterfall Stats):")
        for field_name in ['image_Full', 'image_Ex_large', 'image_large', 'image_medium', 'image_small']:
            count = stats['container_field_usage'].get(field_name, 0)
            if count > 0:
                percentage = (count / stats['images_imported'] * 100) if stats['images_imported'] > 0 else 0
                self.stdout.write(f"   {field_name:20} - {count:4} images ({percentage:.1f}%)")
        
        if stats['errors']:
            self.stdout.write(f"\n‚ö†Ô∏è  Errors ({len(stats['errors'])}):")
            for error in stats['errors'][:10]:
                self.stdout.write(f"   - {error}")
            if len(stats['errors']) > 10:
                self.stdout.write(f"   ... and {len(stats['errors']) - 10} more")
        
        self.stdout.write("")
        self.stdout.write("=" * 80)

